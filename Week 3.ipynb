{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MarsRover:\n",
    "    def __init__(self, transition_probabilities=np.ones((5,2)), rewards=[1, 0, 0, 0, 10], horizon=10):\n",
    "        self.rewards = rewards\n",
    "        self.probs = transition_probabilities\n",
    "        self.c_steps = 0\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def reset(self):\n",
    "        self.c_steps = 0\n",
    "        self.position = 2\n",
    "        return self.position\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        self.c_steps += 1\n",
    "        follow_action = np.random.choice([0, 1], p=[1-self.probs[self.position][action],self.probs[self.position][action]])\n",
    "        if not follow_action:\n",
    "            action = 1 - action\n",
    "\n",
    "        if action == 0:\n",
    "            if self.position > 0:\n",
    "                self.position -= 1\n",
    "        elif action == 1:\n",
    "            if self.position < 4:\n",
    "                self.position += 1\n",
    "        else:\n",
    "            print(\"Not a valid action\")\n",
    "            return\n",
    "        reward = self.rewards[self.position]\n",
    "        return self.position, reward, self.c_steps >= self.horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming_step(v, pi, transition_probabilities, rewards, gamma=0.9):\n",
    "    new_v = np.copy(v)\n",
    "    for s in range(5):\n",
    "        action = pi[s]\n",
    "        next_state = min(4, max(0, s + action + (action-1)))\n",
    "        alternate_state = min(4, max(0, s - action + np.abs(action-1)))\n",
    "        new_v[s] = rewards[next_state] + gamma * (transition_probabilities[s][action]*v[next_state]+(1-transition_probabilities[s][action])*v[alternate_state])\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_step(v, first_visits, total_returns, sample, gamma=0.9):\n",
    "    v_new = np.copy(v)\n",
    "    updated_visits = np.copy(first_visits)\n",
    "    updated_returns = np.copy(total_returns)\n",
    "    visited_this_episode = np.zeros(5)\n",
    "    for i in range(len(sample)):\n",
    "        if i%3 == 0 and not visited_this_episode[sample[i]]:\n",
    "            updated_visits[sample[i]] += 1\n",
    "            future_rewards = [sample[j] for j in range(i+2, len(sample), 3)]\n",
    "            acc_future_rewards = 0\n",
    "            for k in range(len(future_rewards)):\n",
    "                acc_future_rewards += (gamma ** k) * future_rewards[k] \n",
    "            updated_returns[sample[i]] += acc_future_rewards\n",
    "            v_new[sample[i]] = updated_returns[sample[i]] / updated_visits[sample[i]]\n",
    "            visited_this_episode[sample[i]] = 1\n",
    "    return v_new, updated_visits, updated_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero_step(v, sample, alpha=0.1, gamma=0.9):\n",
    "    v_new = np.copy(v)\n",
    "    state = sample[0]\n",
    "    reward = sample[2]\n",
    "    next_state = sample[3]\n",
    "    \n",
    "    v_new[state] = v[state] + alpha * (reward + gamma * v[next_state] - v[state])\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.random.randint(2, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities=np.ones((5, 2))\n",
    "rewards=[1, 0, 0, 0, 10]\n",
    "env = MarsRover(transition_probabilities=transition_probabilities, rewards=rewards)\n",
    "\n",
    "mars_rover_samples = []\n",
    "mars_rover_episodes = []\n",
    "for _ in range(500):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    ep = [state]\n",
    "    while not done:\n",
    "        action = pi[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        mars_rover_samples.append([state, action, reward, next_state])\n",
    "        ep.append(action)\n",
    "        ep.append(reward)\n",
    "        ep.append(next_state)\n",
    "        state = next_state\n",
    "    mars_rover_episodes.append(ep)\n",
    "np.random.shuffle(mars_rover_samples)\n",
    "np.random.shuffle(mars_rover_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "v_dp = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## The value function in dynamic programming\n",
       "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
       "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
       "|    V by DP    |   43.3755  |    46.153800000000004     |   96.47999999999999         |     102.38050000000001       | 124.26299999999999      |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## For policy [1 1 1 1 0]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v_dp = dynamic_programming_step(v_dp, pi, transition_probabilities, rewards)\n",
    "table = Markdown(\"\"\"\n",
    "## The value function in dynamic programming\n",
    "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
    "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
    "|    V by DP    |   {v_dp[0]}  |    {v_dp[1]}     |   {v_dp[2]}         |     {v_dp[3]}       | {v_dp[4]}      |\n",
    "\"\"\".format(v_dp=v_dp))\n",
    "display(table)\n",
    "display(Markdown(\"\"\"## For policy {pi}\"\"\".format(pi=pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "v_mc = np.zeros(5)\n",
    "first_visits = np.zeros(5)\n",
    "total_returns = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Sample episode used to update: [2, 1, 0, 3, 1, 10, 4, 0, 0, 3, 1, 10, 4, 0, 0, 3, 1, 10, 4, 0, 0, 3, 1, 10, 4, 0, 0, 3, 1, 10, 4]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## The value function in MC\n",
       "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
       "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
       "|    V by MC    |   0.0  |    0.0     |   30.852073890000003         |     34.2800821       | 26.977869000000002      |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## For policy [1 1 1 1 0]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(len(mars_rover_episodes))\n",
    "sample_episode = mars_rover_episodes[random_index]\n",
    "display(Markdown(f\"\"\"## Sample episode used to update: {sample_episode}\"\"\"))\n",
    "v_mc, first_visits, total_returns = monte_carlo_step(v_mc, first_visits, total_returns, sample_episode)\n",
    "table = Markdown(\"\"\"\n",
    "## The value function in MC\n",
    "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
    "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
    "|    V by MC    |   {v_mc[0]}  |    {v_mc[1]}     |   {v_mc[2]}         |     {v_mc[3]}       | {v_mc[4]}      |\n",
    "\"\"\".format(v_mc=v_mc))\n",
    "display(table)\n",
    "display(Markdown(\"\"\"## For policy {pi}\"\"\".format(pi=pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "v_td = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Sample transition used to update: [3, 1, 10, 4]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## The value function in TD(0)\n",
       "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
       "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
       "|    V by TD(0) |   0.0  |    0.0     |   0.3249         |     2.72539       | 0.171      |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## For policy [1 1 1 1 0]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(len(mars_rover_samples))\n",
    "sample_transition = mars_rover_samples[random_index]\n",
    "display(Markdown(f\"\"\"## Sample transition used to update: {sample_transition}\"\"\"))\n",
    "v_td = td_zero_step(v_td, sample_transition)\n",
    "table = Markdown(\"\"\"\n",
    "## The value function in TD(0)\n",
    "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
    "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
    "|    V by TD(0) |   {v_td[0]}  |    {v_td[1]}     |   {v_td[2]}         |     {v_td[3]}       | {v_td[4]}      |\n",
    "\"\"\".format(v_td=v_td))\n",
    "display(table)\n",
    "display(Markdown(\"\"\"## For policy {pi}\"\"\".format(pi=pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "v_dp = np.zeros(5)\n",
    "v_mc = np.zeros(5)\n",
    "first_visits = np.zeros(5)\n",
    "total_returns = np.zeros(5)\n",
    "v_td = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Comparing value function development for policy [1 1 1 1 0]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"# Comparing value function development for policy {pi}\"\"\".format(pi=pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Comparing value function development\n",
       "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
       "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
       "|    V by DP    |   21.852073890000003  |    24.2800821     |   30.852073890000003         |     34.2800821       | 30.852073890000003      |\n",
       "|    V by MC    |   0.0  |    0.0     |   30.852073890000003         |     34.28008210000001       | 26.977869000000005      |\n",
       "|    V by TD(0) |   0.0  |    0.0     |   7.326740023744119         |     21.32720889404298       | 15.57925213411243      |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v_dp = dynamic_programming_step(v_dp, pi, transition_probabilities, rewards)\n",
    "\n",
    "random_index = np.random.randint(len(mars_rover_episodes))\n",
    "sample_episode = mars_rover_episodes[random_index]\n",
    "v_mc, first_visits, total_returns = monte_carlo_step(v_mc, first_visits, total_returns, sample_episode)\n",
    "\n",
    "for _ in range(10):\n",
    "    random_index = np.random.randint(len(mars_rover_samples))\n",
    "    sample_transition = mars_rover_samples[random_index]\n",
    "    v_td = td_zero_step(v_td, sample_transition)\n",
    "\n",
    "table = Markdown(\"\"\"\n",
    "## Comparing value function development\n",
    "|               | Good view | Nothing interesting | Nothing interesting | Nothing interesting | Very important science |\n",
    "|:-------------:|:---------:|:-------------------:|:-------------------:|:-------------------:|:----------------------:| \n",
    "|    V by DP    |   {v_dp[0]}  |    {v_dp[1]}     |   {v_dp[2]}         |     {v_dp[3]}       | {v_dp[4]}      |\n",
    "|    V by MC    |   {v_mc[0]}  |    {v_mc[1]}     |   {v_mc[2]}         |     {v_mc[3]}       | {v_mc[4]}      |\n",
    "|    V by TD(0) |   {v_td[0]}  |    {v_td[1]}     |   {v_td[2]}         |     {v_td[3]}       | {v_td[4]}      |\n",
    "\"\"\".format(v_dp=v_dp, v_mc=v_mc, v_td=v_td))\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
